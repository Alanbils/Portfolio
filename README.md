# Modern Data Engineering Portfolio

This repository showcases a production-grade data engineering platform implementing modern data stack practices and cloud-native architectures on AWS.

## ğŸ—ï¸ Repository Structure

```
portfolio/
â”œâ”€â”€ infrastructure/           # Infrastructure as Code (IaC)
â”‚   â”œâ”€â”€ terraform/           # Reusable Terraform modules
â”‚   â”‚   â”œâ”€â”€ modules/        # Core infrastructure modules
â”‚   â”‚   â”‚   â”œâ”€â”€ data-lake/  # S3, Glue, Athena setup
â”‚   â”‚   â”‚   â”œâ”€â”€ streaming/  # Kinesis setup
â”‚   â”‚   â”‚   â””â”€â”€ warehouse/  # Redshift setup
â”‚   â”‚   â””â”€â”€ live/          # Live infrastructure configs
â”‚   â””â”€â”€ terragrunt/         # Environment configurations
â”‚       â”œâ”€â”€ dev/           # Development environment
â”‚       â”œâ”€â”€ staging/       # Staging environment
â”‚       â””â”€â”€ prod/          # Production environment
â”œâ”€â”€ data-pipelines/         # Data Processing Components
â”‚   â”œâ”€â”€ dbt/               # dbt transformations
â”‚   â”‚   â”œâ”€â”€ models/        # Data models (staging, marts)
â”‚   â”‚   â”œâ”€â”€ tests/         # Data quality tests
â”‚   â”‚   â””â”€â”€ docs/          # Documentation
â”‚   â”œâ”€â”€ airflow/           # Airflow DAGs
â”‚   â”‚   â”œâ”€â”€ dags/          # Pipeline definitions
â”‚   â”‚   â””â”€â”€ plugins/       # Custom operators
â”‚   â””â”€â”€ streaming/         # Real-time processing
â”‚       â””â”€â”€ processors/    # Stream processors
â”œâ”€â”€ data-quality/          # Data Quality Framework
â”‚   â”œâ”€â”€ great_expectations/ # Data validation
â”‚   â””â”€â”€ dbt_tests/         # Custom dbt tests
â”œâ”€â”€ monitoring/            # Observability Stack
â”‚   â”œâ”€â”€ dashboards/        # CloudWatch dashboards
â”‚   â””â”€â”€ alerts/           # Alert configurations
â””â”€â”€ notebooks/            # Analysis & Prototyping
    â””â”€â”€ quality/          # Data quality notebooks
```

## ğŸš€ Key Features

### Data Processing
- **Modern Data Stack**: dbt for transformations, Airflow for orchestration
- **Data Lake**: S3-based with Glue catalog and Athena querying
- **Data Warehouse**: Redshift optimization and best practices
- **Streaming Pipeline**: Real-time processing with Kinesis
- **ETL Framework**: AWS Glue, Lambda, and Step Functions

### Infrastructure & DevOps
- **IaC**: Terraform modules with Terragrunt for multi-environment management
- **CI/CD**: Automated testing, documentation, and deployment
- **Security**: IAM roles, encryption, and secure credential management
- **Monitoring**: CloudWatch dashboards and alerts

### Data Quality & Governance
- **Testing**: dbt tests and Great Expectations
- **Documentation**: Auto-generated data catalogs
- **Observability**: Real-time quality monitoring

## ğŸ› ï¸ Technologies

- **Cloud**: AWS (S3, Glue, Athena, Redshift, Kinesis, Lambda)
- **Processing**: dbt, Apache Spark, Python
- **Orchestration**: Airflow
- **DevOps**: Terraform, GitHub Actions
- **Quality**: Great Expectations, dbt Testing

## ğŸ“š Getting Started

1. **Infrastructure Setup**
```bash
cd infrastructure/terragrunt/dev
terragrunt run-all apply
```

2. **Data Pipeline Development**
```bash
cd data-pipelines/dbt
dbt deps
dbt run
```

3. **Quality Checks**
```bash
cd data-quality/great_expectations
great_expectations checkpoint run
```

## ğŸ“Š Project Components

### /infrastructure
- Terraform modules for AWS resources
- Environment-specific configurations
- Network and security settings

### /data-pipelines
- dbt models and transformations
- Airflow DAG definitions
- Streaming pipeline configurations

### /data-quality
- Data quality test suites
- Validation frameworks
- Quality monitoring

### /monitoring
- CloudWatch dashboards
- Alert configurations
- Logging setup

### /notebooks
- Data analysis notebooks
- Pipeline prototypes
- Documentation examples

## ğŸ”„ Data Flow

1. Raw data lands in S3 data lake from various sources
2. AWS Glue catalogs the data and makes it available to query engines
3. Raw data is loaded into Redshift for high-performance analytics
4. dbt models transform raw data into:
   - Clean and validated staging models
   - Core business entities (dimensions and facts)
   - Department-specific analytics models

## ğŸ“– Documentation

Detailed documentation for each component is available in the `/docs` directory:
- Architecture diagrams
- Setup guides
- Best practices
- Troubleshooting guides

## âœ¨ Best Practices

This repository follows these best practices:
- **DRY code** using Terragrunt and modules
- **State management** with remote state in S3
- **Security** with least privilege principles
- **Testing** data quality with dbt and Great Expectations
- **CI/CD** for both infrastructure and analytics code
- **Documentation** as code
