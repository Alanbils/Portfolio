name: "Spark CI"

on:
  push:
    branches:
      - main
    paths:
      - 'src/data/processing/**'
  pull_request:
    paths:
      - 'src/data/processing/**'

jobs:
  lint-python:
    name: "Lint Python"
    runs-on: ubuntu-latest
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v3
        
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.8'
          
      - name: Install Python dependencies
        run: |
          python -m pip install --upgrade pip
          pip install flake8 black isort mypy
          
      - name: Run Black
        run: |
          black --check src/data/processing/
          
      - name: Run Flake8
        run: |
          flake8 src/data/processing/ --count --select=E9,F63,F7,F82 --show-source --statistics
          
      - name: Run isort
        run: |
          isort --check --profile black src/data/processing/
          
  test-python:
    name: "Test Python"
    runs-on: ubuntu-latest
    needs: lint-python
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v3
        
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.8'
          
      - name: Set up JDK 11
        uses: actions/setup-java@v3
        with:
          java-version: '11'
          distribution: 'temurin'
          
      - name: Install PySpark
        run: |
          python -m pip install --upgrade pip
          pip install pytest pytest-cov pyspark==3.1.2 pandas
          
      - name: Create test directory structure
        run: |
          mkdir -p /tmp/data/raw
          mkdir -p /tmp/data/processed
          
      - name: Run tests
        run: |
          cd src/data/processing
          pytest
          
  spark-submit-test:
    name: "Spark Submit Test"
    runs-on: ubuntu-latest
    needs: test-python
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v3
        
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.8'
          
      - name: Set up JDK 11
        uses: actions/setup-java@v3
        with:
          java-version: '11'
          distribution: 'temurin'
          
      - name: Install PySpark
        run: |
          python -m pip install --upgrade pip
          pip install pyspark==3.1.2 pandas
          
      - name: Create test data
        run: |
          mkdir -p /tmp/spark-test/raw
          mkdir -p /tmp/spark-test/processed
          
          # Create sample data
          cat > /tmp/spark-test/create_test_data.py << EOF
          from pyspark.sql import SparkSession
          from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType, DateType
          from datetime import date
          
          # Create a Spark session
          spark = SparkSession.builder.appName("CreateTestData").getOrCreate()
          
          # Define the schema for orders data
          schema = StructType([
              StructField("order_id", StringType(), False),
              StructField("customer_id", IntegerType(), True),
              StructField("order_date", DateType(), True),
              StructField("order_status", StringType(), True),
              StructField("order_total", DoubleType(), True)
          ])
          
          # Create sample data
          data = [
              ("ORD001", 1001, date(2023, 1, 15), "completed", 125.99),
              ("ORD002", 1002, date(2023, 1, 16), "processing", 85.50),
              ("ORD003", 1001, date(2023, 1, 20), "completed", 45.25),
              ("ORD004", 1003, date(2023, 1, 22), "cancelled", 210.75),
              ("ORD005", 1004, date(2023, 1, 25), "completed", 35.00),
              ("ORD006", None, date(2023, 1, 27), None, 75.25),
              ("ORD007", 1005, date(2023, 1, 30), "completed", 190.50)
          ]
          
          # Create a DataFrame
          orders_df = spark.createDataFrame(data, schema)
          
          # Write to parquet
          orders_df.write.parquet("/tmp/spark-test/raw/orders")
          
          spark.stop()
          EOF
          
          python /tmp/spark-test/create_test_data.py
          
      - name: Run Spark job
        run: |
          cd src/data/processing
          spark-submit spark/orders_transformer.py /tmp/spark-test/raw/orders /tmp/spark-test/processed/orders
          
      - name: Verify output
        run: |
          # Quick check to make sure output files were created
          if [ ! -d "/tmp/spark-test/processed/orders" ]; then
            echo "Output directory does not exist!"
            exit 1
          fi
          
          # Count the number of files in the output directory
          OUTPUT_FILES=$(find /tmp/spark-test/processed/orders -name "*.parquet" | wc -l)
          if [ "$OUTPUT_FILES" -eq 0 ]; then
            echo "No output files were created!"
            exit 1
          fi
          
          echo "Spark job completed successfully with $OUTPUT_FILES output files."